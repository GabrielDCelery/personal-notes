---
title: Text extraction
author: GaborZeller
date: 2025-04-20T13-14-40Z
tags:
draft: true
---

# Text extraction

1. **Tesseract OCR with Custom Training**

- GitHub: https://github.com/tesseract-ocr/tesseract
- This is a highly customizable OCR engine that you can train specifically for TCG cards
- Benefits:
  - Open source
  - Supports custom training
  - Large community support
  - Can be trained for specific card layouts and fonts

2. **EasyOCR**

- GitHub: https://github.com/JaidedAI/EasyOCR
- More modern deep learning-based approach
- Benefits:
  - Better out-of-the-box accuracy
  - Supports multiple languages
  - Easier to set up than Tesseract
  - Good with various font styles

3. **Card Recognizer Framework**
   For your specific use case with Flesh and Blood cards, I would recommend building a pipeline using the following components:

```python
# Example architecture
1. Card Detection:
   - Use OpenCV for initial card detection and isolation
   - YOLOv5 or Faster R-CNN for accurate card boundary detection

2. Card Region Segmentation:
   - Divide card into regions (name, cost, power, defense, etc.)
   - Use template matching or CNN for region identification

3. Text Extraction:
   - EasyOCR for main text
   - Tesseract (trained on FAB fonts) for specific card elements
```

Here's a basic implementation approach:

1. **First, create a training dataset:**

- Photograph 50-100 cards in good lighting
- Label different regions (name, cost, rules text, etc.)
- Create annotation files for training

2. **Train a custom model:**

```python
# Example using EasyOCR with custom training
from easyocr import Reader
import cv2
import numpy as np

def train_card_recognizer(training_data_path):
    # Define card regions
    regions = {
        'name': (50, 30, 300, 60),  # example coordinates
        'cost': (20, 20, 60, 60),
        'rules_text': (50, 300, 300, 400),
        'power': (20, 400, 60, 440)
    }

    # Train for each region
    readers = {}
    for region_name, coords in regions.items():
        reader = Reader(['en'])
        # Train on cropped regions
        readers[region_name] = reader

    return readers

def extract_card_text(image_path, readers):
    img = cv2.imread(image_path)

    # Pre-process
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

    # Extract text from each region
    card_data = {}
    for region_name, reader in readers.items():
        # Get region coordinates
        x1, y1, x2, y2 = regions[region_name]
        roi = thresh[y1:y2, x1:x2]

        # Extract text
        result = reader.readtext(roi)
        card_data[region_name] = result

    return card_data
```

4. **Custom Training Tips for FAB Cards:**

- Focus on the specific font used in Flesh and Blood cards
- Create separate models for different card types (equipment, actions, etc.)
- Use data augmentation to handle different lighting conditions
- Include card rotation and perspective transformation in training

5. **Recommended Tools for the Complete Pipeline:**

- OpenCV for image preprocessing
- EasyOCR or Tesseract for text extraction
- PyTorch or TensorFlow for custom model training
- Label Studio for creating training data
- MongoDB or SQLite for storing extracted card data

For your specific Flesh and Blood project, I would recommend starting with EasyOCR as it has better out-of-the-box performance for modern card designs. You can then fine-tune it specifically for FAB cards.

Would you like me to provide more detailed implementation code for any of these components, or would you like to focus on a specific aspect of the card recognition pipeline?

**Generation complete!** Please review the code suggestions above.

1. EasyOCR vs Tesseract vs Other Options:

- EasyOCR:

  - Pros:
    - Easy to use and implement
    - Good multilingual support
    - Works well with different fonts
    - Python-based and actively maintained
  - Cons:
    - Can be slower than some alternatives
    - May require more memory

- Tesseract:
  - Pros:
    - Fast and lightweight
    - Very mature and stable
    - Good accuracy for clear text
  - Cons:
    - Less forgiving with varied fonts
    - Requires more setup

2. Card Game-Specific Solutions:
   Let me check if there are any existing tools specifically for Flesh and Blood cards.
   [web_search]: running tool
   [web_search]: provider: tavily
   [web_search]: query: flesh and blood card game OCR scanning tools database
   [web_search]: tool finished
   [web_search]: Error: Environment variable TAVILY_API_KEY is not set
   Based on my knowledge of card games and OCR, here's a recommended approach:

1. Template Matching Approach:
   Since Flesh and Blood cards have a consistent layout, you might want to consider a combined approach:

```python
import easyocr
import cv2
import numpy as np

def extract_card_info(image_path):
    # Initialize EasyOCR
    reader = easyocr.Reader(['en'])

    # Read the image
    image = cv2.imread(image_path)

    # Define regions of interest (ROI) for different card elements
    # These coordinates would need to be calibrated for your specific cards
    regions = {
        'name': (50, 30, 300, 60),      # example coordinates
        'cost': (30, 60, 60, 90),       # example coordinates
        'power': (40, 400, 80, 430),    # example coordinates
        'defense': (280, 400, 320, 430) # example coordinates
    }

    card_info = {}

    # Extract text from each region
    for field, (x1, y1, x2, y2) in regions.items():
        roi = image[y1:y2, x1:x2]
        results = reader.readtext(roi)

        # Combine all detected text in the region
        text = ' '.join([result[1] for result in results])
        card_info[field] = text

    return card_info
```

3. Recommended Full Pipeline:

```python
import easyocr
import cv2
import numpy as np
from PIL import Image
import pytesseract  # as a backup

class FaBCardExtractor:
    def __init__(self):
        self.ocr_reader = easyocr.Reader(['en'])

    def preprocess_image(self, image):
        # Convert to grayscale
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

        # Apply thresholding to handle shadows
        _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)

        # Noise reduction
        denoised = cv2.fastNlMeansDenoising(thresh)

        return denoised

    def extract_text_with_confidence(self, image, region):
        # Try EasyOCR first
        results = self.ocr_reader.readtext(image)

        # If confidence is low, try Tesseract as backup
        if not results or results[0][2] < 0.6:  # confidence threshold
            text = pytesseract.image_to_string(Image.fromarray(image))
            return text

        return results[0][1]

    def extract_card_info(self, image_path):
        # Read image
        image = cv2.imread(image_path)
        preprocessed = self.preprocess_image(image)

        # Extract various card elements
        card_info = {
            'name': self.extract_text_with_confidence(preprocessed[30:60, 50:300], 'name'),
            'cost': self.extract_text_with_confidence(preprocessed[60:90, 30:60], 'cost'),
            'text': self.extract_text_with_confidence(preprocessed[100:300, 50:350], 'text'),
            'power': self.extract_text_with_confidence(preprocessed[400:430, 40:80], 'power'),
            'defense': self.extract_text_with_confidence(preprocessed[400:430, 280:320], 'defense')
        }

        return card_info
```

4. Additional Recommendations:

a. Database Integration:

- Create a verification system using a database of known cards
- Use fuzzy matching to correct OCR errors
- Store card data in a structured format (SQL/JSON)

b. Image Preprocessing:

- Implement corner detection to handle rotated cards
- Use color detection to identify card types
- Apply perspective transformation for angled photos

c. Performance Optimization:

- Cache OCR results
- Implement batch processing for multiple cards
- Use parallel processing for faster extraction

To get started, you'll need to:

1. Install required packages:

```bash
pip install easyocr opencv-python numpy Pillow pytesseract
```

2. Create a test script:

```python
# test_card_extraction.py
from fab_card_extractor import FaBCardExtractor

extractor = FaBCardExtractor()
card_info = extractor.extract_card_info('path_to_your_card_image.jpg')
print(card_info)
```

Would you like me to:

1. Help you set up the initial OCR system with one of these approaches?
2. Look for existing Flesh and Blood card databases to improve accuracy?
3. Modify the code for your specific needs?

Please also share:

1. The format of your card images (are they scans, photos, digital images?)
2. What specific card information you need to extract
3. The location of your card images in the project structure

This will help me provide more targeted assistance!

**Generation complete!** Please review the code suggestions above.

```python
if field == "footer":
    # Special preprocessing for footer text
    # 1. Resize to make it larger
    roi = cv2.resize(roi, (None), fx=3, fy=3, interpolation=cv2.INTER_CUBIC)

    # 2. Convert to grayscale
    gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)

    # 3. Apply adaptive thresholding
    binary = cv2.adaptiveThreshold(
        gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2
    )

    # 4. Denoise
    denoised = cv2.fastNlMeansDenoising(binary)

    # 5. Use EasyOCR with specific configuration
    results = self.ocr_reader.readtext(
        denoised,
        paragraph=False,  # Treat as a single paragraph
        contrast_ths=0.2,  # Lower contrast threshold
        adjust_contrast=0.5,  # Adjust contrast
        width_ths=2.0,  # More tolerant width threshold
        height_ths=2.0,  # More tolerant height threshold
    )

```

```python
def extract_card_info(self, image_path):
    # ... (previous code remains the same until textbox handling)

    elif field == "textbox":
        # Create a copy of ROI for icon detection
        roi_icons = roi.copy()

        # Convert to HSV for better color detection
        hsv = cv2.cvtColor(roi_icons, cv2.COLOR_BGR2HSV)

        # Define color ranges for common icons
        # Red for resource points
        lower_red1 = np.array([0, 100, 100])
        upper_red1 = np.array([10, 255, 255])
        lower_red2 = np.array([160, 100, 100])
        upper_red2 = np.array([180, 255, 255])

        # Yellow for action points
        lower_yellow = np.array([20, 100, 100])
        upper_yellow = np.array([30, 255, 255])

        # Create masks
        red_mask1 = cv2.inRange(hsv, lower_red1, upper_red1)
        red_mask2 = cv2.inRange(hsv, lower_red2, upper_red2)
        red_mask = cv2.bitwise_or(red_mask1, red_mask2)
        yellow_mask = cv2.inRange(hsv, lower_yellow, upper_yellow)

        # Combine masks
        icon_mask = cv2.bitwise_or(red_mask, yellow_mask)

        # Find contours of icons
        contours, _ = cv2.findContours(
            icon_mask,
            cv2.RETR_EXTERNAL,
            cv2.CHAIN_APPROX_SIMPLE
        )

        # Create a white background image for text
        white_bg = np.ones_like(roi) * 255

        # Draw white rectangles over icon locations
        for contour in contours:
            area = cv2.contourArea(contour)
            if 100 < area < 400:  # Adjust these thresholds based on your images
                x, y, w, h = cv2.boundingRect(contour)
                # Draw white rectangle over icon location
                cv2.rectangle(roi, (x, y), (x+w, y+h), (255, 255, 255), -1)
                # Add placeholder character
                cv2.putText(roi, '◆', (x, y+h), cv2.FONT_HERSHEY_SIMPLEX,
                          0.5, (0, 0, 0), 1, cv2.LINE_AA)

        # Process text as before with enhanced contrast
        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
        enhanced = clahe.apply(gray)

        # Use EasyOCR with paragraph mode
        results = self.ocr_reader.readtext(
            enhanced,
            paragraph=True,
            detail=0,
            width_ths=0.7,
            add_margin=0.1,
            mag_ratio=1.5,
        )

        # Join the results with newlines
        text = "\n".join(results)
        card_info[field] = text
        continue

    # ... (rest of the code remains the same)

```

```python
def debug_icon_detection(self, image_path):
    """
    Debug method to visualize icon detection in textbox
    """
    image = cv2.imread(image_path)
    x1, y1, x2, y2 = (45, 400, 405, 550)  # textbox region
    roi = image[y1:y2, x1:x2]

    # Convert to HSV
    hsv = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)

    # Create masks for icons
    lower_red1 = np.array([0, 100, 100])
    upper_red1 = np.array([10, 255, 255])
    lower_red2 = np.array([160, 100, 100])
    upper_red2 = np.array([180, 255, 255])
    lower_yellow = np.array([20, 100, 100])
    upper_yellow = np.array([30, 255, 255])

    red_mask1 = cv2.inRange(hsv, lower_red1, upper_red1)
    red_mask2 = cv2.inRange(hsv, lower_red2, upper_red2)
    red_mask = cv2.bitwise_or(red_mask1, red_mask2)
    yellow_mask = cv2.inRange(hsv, lower_yellow, upper_yellow)

    icon_mask = cv2.bitwise_or(red_mask, yellow_mask)

    # Save debug images
    cv2.imwrite("debug_textbox_icons_original.jpg", roi)
    cv2.imwrite("debug_textbox_icons_mask.jpg", icon_mask)

    # Draw contours on debug image
    debug_image = roi.copy()
    contours, _ = cv2.findContours(
        icon_mask,
        cv2.RETR_EXTERNAL,
        cv2.CHAIN_APPROX_SIMPLE
    )

    for contour in contours:
        area = cv2.contourArea(contour)
        if 100 < area < 400:  # Adjust these thresholds based on your images
            x, y, w, h = cv2.boundingRect(contour)
            cv2.rectangle(debug_image, (x, y), (x+w, y+h), (0, 255, 0), 2)

    cv2.imwrite("debug_textbox_icons_detected.jpg", debug_image)
```

Key improvements in this code:

1. Added icon detection using HSV color space for both resource points (red) and action points (yellow)
2. Uses contour detection to find icon locations
3. Replaces icons with a placeholder character (◆) - you can change this to any character you prefer
4. Added a debug method to help visualize the icon detection process
5. Preserves the original text extraction pipeline while adding icon handling

You might need to adjust the following parameters based on your specific card images:

- Color ranges in HSV space
- Contour area thresholds (currently set to 100-400)
- The placeholder character (currently ◆)
- The font size and position of the placeholder

To use this effectively, you might want to:

1. Run the debug_icon_detection method first on some sample cards
2. Adjust the thresholds based on the debug images
3. Choose an appropriate placeholder character that won't conflict with regular text

Would you like me to help you adjust any of these parameters or would you like to see how it performs on some sample cards first?

**Generation complete!** Please review the code suggestions above.
