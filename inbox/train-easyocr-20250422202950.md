---
title: Train easyocm
author: GaborZeller
date: 2025-04-22T20-29-50Z
tags:
draft: true
---

# Train EasyOCR

1. Using EasyOCR's training functionality:

```python
from easyocr.trainer import train

# Basic training configuration
train(
    lang='en',
    train_data_path='path/to/training/data',
    val_data_path='path/to/validation/data',
    model_path='path/to/save/model',
    trainer_config={
        'batch_size': 32,
        'epochs': 1000,
        'learning_rate': 0.0001,
    }
)
```

2. Prepare training data:

- Create a dataset with images and their corresponding ground truth text
- Format the data as required by EasyOCR:

```
# label.txt format
image_path\tlabel
image1.jpg\ttext1
image2.jpg\ttext2
```

3. Detailed training example:

```python
import torch
from easyocr.trainer import train

trainer_config = {
    'batch_size': 32,
    'epochs': 1000,
    'learning_rate': 0.0001,
    'optimizer': 'adam',
    'scheduler': 'cosine',
    'weight_decay': 0.0001,
    'validation_interval': 100,
    'checkpoint_interval': 1000,
}

train_config = {
    'lang': 'en',
    'train_data_path': 'path/to/train/label.txt',
    'val_data_path': 'path/to/val/label.txt',
    'output_path': 'path/to/output',
    'model_prefix': 'card_text_recognition',
    'saved_model': None,  # path to pretrained model if fine-tuning
    'trainer_config': trainer_config,
    'transform_config': None  # optional image transformations
}

train(**train_config)
```

4. Data augmentation to improve training:

```python
import albumentations as A

transform_config = A.Compose([
    A.RandomBrightnessContrast(p=0.5),
    A.GaussNoise(p=0.3),
    A.Rotate(limit=5, p=0.5),
    A.RandomScale(scale_limit=0.1, p=0.5),
])
```

5. Using the trained model:

```python
import easyocr

reader = easyocr.Reader(['en'], model_storage_directory='path/to/model/directory',
                       user_network_directory='path/to/trained/model')
result = reader.readtext('card_image.jpg')
```

Tips for better results:

1. Preprocessing images:

```python
import cv2
import numpy as np

def preprocess_image(image):
    # Convert to grayscale
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

    # Apply thresholding
    _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)

    # Denoise
    denoised = cv2.fastNlMeansDenoising(thresh)

    return denoised
```

2. Create a diverse dataset:

- Include different card types
- Various lighting conditions
- Different angles
- Different card conditions (worn, mint)
- Different text styles and fonts

3. Region of Interest (ROI) selection:

```python
def extract_text_region(image):
    # Define region where text typically appears
    height, width = image.shape[:2]
    roi = image[int(height*0.1):int(height*0.9),
               int(width*0.1):int(width*0.9)]
    return roi
```

4. Post-processing results:

```python
def post_process_text(text):
    # Remove unwanted characters
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)

    # Convert to uppercase/lowercase as needed
    text = text.upper()

    # Additional game-specific processing
    return text
```

5. Validation and error handling:

```python
def validate_card_text(text, known_cards_database):
    # Compare with known card names/text
    closest_match = difflib.get_close_matches(text, known_cards_database, n=1)
    if closest_match:
        return closest_match[0]
    return text
```

Remember to:

- Start with a small, well-labeled dataset
- Gradually increase dataset size
- Use cross-validation
- Monitor training progress
- Test thoroughly with different card types
- Consider using data augmentation
- Fine-tune hyperparameters based on results

This should help you create a more accurate OCR model specifically for your trading card game images.
