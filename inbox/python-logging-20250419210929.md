---
title: Python logging
author: GaborZeller
date: 2025-04-19T21-09-29Z
tags:
draft: true
---

# Python logging

I'll provide examples of bad and good logging practices in Python, using real-world scenarios.

# Python Logging Best Practices and Anti-Patterns

## 1. Log Level Usage

❌ Bad Practice - Incorrect Log Levels:

```python
# Using wrong log levels
logger.error("User logged in successfully")  # Wrong! Success is not an error
logger.debug("Database connection failed!")  # Wrong! Critical issues shouldn't be debug
logger.info("Variable x = 42")  # Wrong! Debug-level detail in info
```

✅ Good Practice - Appropriate Log Levels:

```python
# Using appropriate log levels
logger.info("User logged in successfully")  # Business events use INFO
logger.error("Database connection failed: Connection refused")  # Errors use ERROR
logger.debug("Variable x = 42")  # Technical details use DEBUG
logger.warning("API rate limit at 80% capacity")  # Potential issues use WARNING
logger.critical("System out of disk space")  # System-critical issues use CRITICAL
```

## 2. Context in Log Messages

❌ Bad Practice - Poor Context:

```python
# Lacking context
logger.error("Failed")  # What failed?
logger.info("Success")  # What succeeded?
logger.debug("Value is 42")  # Which value?
```

✅ Good Practice - Rich Context:

```python
# Rich context with structured logging
logger.error(
    "Database query failed",
    extra={
        "query_id": query_id,
        "user_id": user_id,
        "error_code": error_code,
        "duration_ms": duration
    }
)

# Good context in message
logger.info(
    f"User {username} successfully logged in from {ip_address}",
    extra={"login_method": "2FA", "user_agent": user_agent}
)

logger.debug(
    f"Cache hit ratio: {hit_ratio:.2f}%",
    extra={"cache_size": current_size, "total_requests": total_reqs}
)
```

## 3. Exception Handling

❌ Bad Practice - Poor Exception Logging:

```python
try:
    process_data()
except Exception as e:
    logger.error(str(e))  # Lost stack trace and context
    # or
    print(f"Error: {e}")  # Using print instead of logger
```

✅ Good Practice - Proper Exception Logging:

```python
try:
    process_data()
except Exception as e:
    logger.exception("Failed to process data")  # Automatically includes traceback
    # or
    logger.error("Data processing failed", exc_info=True, extra={
        "data_id": data_id,
        "process_type": process_type,
        "retry_count": retry_count
    })
```

## 4. Configuration and Setup

❌ Bad Practice - Poor Configuration:

```python
# Inconsistent or global logging configuration
logging.basicConfig(level=logging.INFO)
logging.info("Message")  # Using root logger directly

# Or multiple configurations scattered throughout code
logging.basicConfig(level=logging.DEBUG)
# ... somewhere else in code
logging.basicConfig(level=logging.INFO)  # Won't work, first call wins
```

✅ Good Practice - Proper Configuration:

```python
# In logging_config.py
import logging.config
import yaml

def setup_logging():
    """Configure logging with a YAML config file."""
    with open('logging_config.yaml', 'r') as f:
        config = yaml.safe_load(f)

    logging.config.dictConfig(config)

    # Add correlation ID to all log records
    logging.getLogger().addFilter(CorrelationIdFilter())

# Example logging configuration
config = {
    'version': 1,
    'formatters': {
        'detailed': {
            'format': '%(asctime)s - %(name)s - %(levelname)s - %(correlation_id)s - %(message)s'
        }
    },
    'handlers': {
        'console': {
            'class': 'logging.StreamHandler',
            'formatter': 'detailed',
            'level': 'DEBUG'
        },
        'file': {
            'class': 'logging.handlers.RotatingFileHandler',
            'filename': 'app.log',
            'maxBytes': 1024 * 1024,  # 1MB
            'backupCount': 3,
            'formatter': 'detailed',
            'level': 'INFO'
        }
    },
    'root': {
        'level': 'DEBUG',
        'handlers': ['console', 'file']
    }
}
```

## 5. Custom Logging Enhancements

✅ Good Practice - Custom Logging Features:

```python
import logging
from typing import Any
from uuid import uuid4
from contextvars import ContextVar

# Correlation ID for tracking requests
correlation_id: ContextVar[str] = ContextVar('correlation_id', default='')

class CorrelationIdFilter(logging.Filter):
    """Add correlation ID to all log records."""

    def filter(self, record: logging.LogRecord) -> bool:
        record.correlation_id = correlation_id.get()
        return True

class CustomLogger(logging.Logger):
    """Enhanced logger with structured logging capabilities."""

    def __init__(self, name: str, level: int = logging.NOTSET) -> None:
        super().__init__(name, level)
        self.correlation_id = correlation_id

    def structured_log(
        self,
        level: int,
        msg: str,
        *args: Any,
        **kwargs: Any
    ) -> None:
        """Log with structured data."""
        extra = kwargs.pop('extra', {})
        extra['correlation_id'] = self.correlation_id.get()

        if kwargs:
            extra.update(kwargs)

        self.log(level, msg, *args, extra=extra)

# Usage example
logger = CustomLogger(__name__)

def process_request(request_id: str) -> None:
    correlation_id.set(request_id)
    logger.structured_log(
        logging.INFO,
        "Processing request",
        request_id=request_id,
        user_id=user_id,
        action="process_data"
    )
```

## 6. Applying to Your Image Scraper

Here's how we could improve the logging in your image scraper:

```python
import logging
from typing import Optional
from dataclasses import dataclass
from datetime import datetime

@dataclass
class ImageDownloadMetrics:
    """Metrics for image downloads."""
    start_time: datetime
    end_time: Optional[datetime] = None
    status_code: Optional[int] = None
    file_size: Optional[int] = None

    @property
    def duration_ms(self) -> Optional[float]:
        if self.end_time:
            return (self.end_time - self.start_time).total_seconds() * 1000
        return None

async def download_image(
    session: ClientSession,
    image_url: str,
    download_dir: str
) -> None:
    """Download a single image with enhanced logging."""
    metrics = ImageDownloadMetrics(start_time=datetime.now())
    file_name = image_url.split("/")[-1]

    try:
        file_path = os.path.join(download_dir, file_name)

        if os.path.isfile(file_path):
            logger.info("Skipping existing file", extra={
                "file_name": file_name,
                "file_path": file_path,
                "reason": "already_exists"
            })
            return

        if not image_url.startswith(("http://", "https://")):
            logger.error("Invalid URL scheme", extra={
                "image_url": image_url,
                "file_name": file_name,
                "error_type": "invalid_url_scheme"
            })
            raise InvalidImageURLError(f"Invalid URL scheme: {image_url}")

        async with session.get(image_url) as response:
            metrics.status_code = response.status

            if response.status != 200:
                logger.error("Failed to download image", extra={
                    "status_code": response.status,
                    "image_url": image_url,
                    "file_name": file_name,
                    "headers": dict(response.headers)
                })
                raise ImageDownloadError(
                    f"Failed to download {file_name}. Status: {response.status}"
                )

            content = await response.read()
            metrics.file_size = len(content)

            logger.debug("Downloading image", extra={
                "file_name": file_name,
                "content_type": response.headers.get("content-type"),
                "content_length": response.headers.get("content-length")
            })

            with open(file_path, "wb") as f:
                f.write(content)

            metrics.end_time = datetime.now()
            logger.info("Successfully downloaded image", extra={
                "file_name": file_name,
                "file_size": metrics.file_size,
                "duration_ms": metrics.duration_ms,
                "status_code": metrics.status_code
            })

    except aiohttp.ClientError as e:
        metrics.end_time = datetime.now()
        logger.error("Network error during download", extra={
            "error_type": type(e).__name__,
            "error_message": str(e),
            "image_url": image_url,
            "file_name": file_name,
            "duration_ms": metrics.duration_ms
        }, exc_info=True)
        raise ImageDownloadError(f"Failed to download {image_url}") from e

    except OSError as e:
        metrics.end_time = datetime.now()
        logger.error("File system error", extra={
            "error_type": type(e).__name__,
            "error_message": str(e),
            "file_path": file_path,
            "file_name": file_name,
            "duration_ms": metrics.duration_ms
        }, exc_info=True)
        raise ImageDownloadError(f"Failed to save {file_name}") from e
```

Key Improvements in this Version:

1. Structured logging with consistent extra fields
2. Performance metrics tracking
3. Detailed error context
4. Appropriate log levels for different scenarios
5. Rich context in all log messages
6. Proper exception handling with stack traces
7. Metrics for monitoring and debugging

Remember:

- Use appropriate log levels
- Include relevant context
- Structure your logs consistently
- Track metrics where useful
- Handle exceptions properly
- Use extra fields for machine-readable data
- Keep human-readable messages clear and concise
- Configure logging properly at application startup
- Consider adding correlation IDs for request tracking
- Use custom logging enhancements when needed

This makes logs much more useful for:

- Debugging issues
- Monitoring performance
- Tracking system health
- Auditing operations
- Understanding user behavior
- Identifying patterns and problems
